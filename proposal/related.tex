% !TEX root = proposal.tex
\section{Background and Related Work}
\label{sec:related}

In this section, I discuss related work and present my previous research
results relevant to my thesis to help reader to identify the context of this
thesis proposal.

\subsection{Related Work}

\subsubsection{Inline Monitors}
\label{ssec:inline}

Program protection and profiling using inline monitors are dynamic analysis
approaches that execute specific analysis logics along with the application
process. Instance of the technology includes data flow tracking
(DFT)~\cite{DFT}, memory integrity checking~\cite{memcheck}, control flow
integrity~\cite{cfi}, method counting, call graph profiling and so on.
Given that the technology can be implemented interleaving analysis/monitoring
logics into the program execution, we can choose different instrumentation
targets either of source code or program binaries and use different
instrumentation mediums to implement inline monitors.

Employing source code based approach, we can use representations
exposed~\cite{AST,LLVM-IR} by compiler internals as instrumentation targets or
source-to-source approaches~\cite{txl, cil} to make changes to source code.
This approach comes in reasonable amount of overhead roughly around $2\times$
or less, but it is limited in completeness not being able to support COTS
binaries (\ie 3rd party libraries).  An alternative can be binary
implementation approaches either based on process-wide virtualization using
dynamic binary instrumentation(DBI)~\cite{PIN, dynamoRIO, valgrind} or
system-wide virtualization~\cite{qemu,xen}. These address coverage issue as
these can handle unknown program binaries. However, it comes with excessive
amount of overhead which vary from $\times 5 \sim \times 100$ based on analyses
and application domains. Hardware assisted implementation~\cite{HARD, lba} can
implement inline monitors with minimal amount of overhead less than 5\% at most
supporting full coverage, but the we have not yet seen this functionality
supported by major vendors with their commodity production.

\subsubsection{Data Flow Tracking (DFT)}

DFT is one of inline monitoring approaches that accurately tracks selected data
of interest, as they flow during program execution. Among other uses, DFT has
been employed to provide insight in the behavior of applications and systems,
and to assist in the identification of configuration errors. Most prominently,
it has been used in the security field to defend against various software
exploits~\cite{}, and to enforce information flow by monitoring and restricting
the use of sensitive data~\cite{}. For the former, the network is usually
defined as the source of interesting or “tainted” data, while the use of
tainted data is disallowed in certain program locations (\eg, in instructions
manipulating the control flow of programs, such as indirect branch instructions
and function calls). For the latter, the developer or the user is responsible
for specifying the data that needs to be tracked and the restrictions on their
use. DFT is even used to assist solving performance problems~\cite{} pairing
configuration file entries with performance bottlenecks as sources and sinks. 

DFT is implemented by having shadow context that corresponds to the original
execution context. The shadow context comprise of shadow operations and shadow
memory area. Table~\ref{tab:dft_tracking} shows how instructions' DFT semantics
for general purpose instruction architecture (ISA) are defined to perform DFT
operations to keep track of changes from shadow memory area. 

\begin{table}[h]
        \centering
\begin{tabular}{|l|l|}
\hline
{\bf Instruction} & {\bf Tag propagation rule} \\ \hline \hline
    {\tt \specialcell{ALU-OP OP1 $\leftarrow$ OP2 \\ (add, sub \dots)}} & 
    {\tt t(op1) $\vert=$ t(op2)}\\ \hline
    {\tt MOV OP1  $\leftarrow$  OP2} & {\tt t(op1) = t(op2)}     \\ \hline
    {\tt LOAD OP1 $\leftarrow$ [OP2]} & {\tt t(OP1) = t([OP2])}  \\ \hline
    {\tt STORE [OP1] $\leftarrow$ OP2} & {\tt t([OP1]) = t(OP2)} \\ \hline
\end{tabular}
\caption{presents an interpretation of DFT semantics for pseudo instruction set
architecture.}
\label{tab:dft_tracking}
\end{table}

The specifics of DFT can vary significantly depending on ones goals,
performance considerations, and deployment platform. One possible
classification of existing mechanisms can be made based on the means by which
the tracking logic is augmented on regular program execution. As we discussed
from Section~\ref{ssec:inline}, DFT can be performed by inserting data tracking
logic statically during the compilation of software, or by performing source-
to-source code transformation~\cite{}. It can also be applied dynamically by
augmenting instrumentation code on existing binaries using dynamic binary
instrumentation (DBI)~\cite{}  or a modified virtual machine (VM)~\cite{}.
Finally, DFT can be also performed in hardware~\cite{}.

\subsubsection{Parallelized Analysis}
\label{ssec:parallel}

The idea of decoupling dynamic program analyses from execution, to run them in
parallel, has been studied in past in various contexts~\cite{} . 
%
Aftersight~\cite{}, ReEmu~\cite{}, and Paranoid Android~\cite{} leverage record
and replay for recording execution and replaying it, along with the analysis,
on a remote host or a different CPU (replica). They are mostly geared toward
off-line analyses and can greatly reduce the overhead imposed on the
application.  However, the speed of the analysis itself is not improved, since
execution needs to be replayed and augmented with the analysis code on the
replica.  SuperPin~\cite{} and Speck~\cite{} use speculative execution to run
application and (in-lined) analysis code in multiple threads that execute in
parallel. These systems sacrifice significant processing power to achieve speed
up.  Furthermore, handling multi-threaded applications without hardware support
remains a challenging issue for this approach. CAB~\cite{} and PiPA~\cite{} aim
at offloading the analysis code alone to another execution thread, and they are
the closed to \sreplica. However, neither of the two has been able to deliver
the expected performance gains, due to {\it (i)} naively collecting information
from the application, and {\it (ii)} the high overhead of communicating it to
the analysis thread(s). 

\subsection{Previous Research}

From my previous research, I have addressed the high overhead issue inherent to
heavy-weighted inline monitors such as DFT implementations.

\subsubsection{\libdft}

\libdft is a highly optimized DFT framework that shows comparable to or faster
performance than most previous DFT implementations. \libdft implements
instruction level monitors by instrumenting DFT instruction that perform shadow
operations against {\tt x86} binary stream at runtime using PIN~\cite{} DBI
framework.
%
\libdft's performance gain comes in two folds. 

The first optimization is by having {\it inline-friendly} DFT operations. To
interleave codes from two different contexts, underlying DBI framework should
add management instructions to save and restore states relevant to each context
for every instrumentation. The state includes the whole CPU registers and the
operation of spilling/re-filling these is typically expensive. {\it
Instrumentation inlining}~\cite{} reduces this overhead taking advantage of
remaining register entries unused from the application context. In order to
make DFT operations to be instrumentation suitable for inlining, it needs to
meet the following two conditions.
%
\begin{enumerate} \item The instruction count for DFT operation should less
than ten.  \item DFT operation should not contain branch operations that make
updates to {\tt EFLAG} register.  \end{enumerate} 
%
Having highly crafted routines that satisfy above conditions, \libdft could
have most of its operations in-lined reducing signification amount of overhead.

The second optimization is by having optimal shadow memory design that
minimizes the cycles needed to translate real address entries into shadow
memory counterparts. We have number of design choices regarding shadow store
structure that come with CPU time vs. memory space trade-offs. Not to reduce
the execution transparency by assign too large portion of address space, most
of shadow memory architectures leverage multiple level of indirections which
inevitably involve conditional operations. \libdft avoids this problem by
instrumentation every memory routines to co-allocate/deallocate counterpart
shadow entries. This not only reduce the instruction counts but also eliminate
need for control instruction in translations.

\libdft takes the form of a shared library enabling developers to create of
DFT-enabled Pintools for binaries, using its extensive API. For example,
\libdft already includes a DTA tool, which can be used to protect applications
from remote buffer overflow exploits.

\subsubsection{\tfa} 

Main insights behind \tfa is that update-to-date DFT are lacking in {\it i)}
consideration for global context {\it ii)} understanding of DFT operation
semantics which is different from the original execution semantics.  \tfa
attempts to address these issues by dedicating off-line static phase that
performs application and DFT specific analysis. 

\begin{figure}[tb]
    \centering
%        \includegraphics[width=\linewidth]{figs/execution_model.pdf}
    \includegraphics[width=0.65\linewidth]{figs/overview_model.eps}
    \caption{\tfa overview: It extract basic blocks and control flow
    information using a combination of dynamic and static analysis, and then
    statically analyze this information to produce optimized data tracking
    code.
   \label{fig:approach_overview}}
\end{figure}

Figure~\ref{fig:approach_overview} shows a high-level overview of our approach
for optimizing data tracking. We start by dynamically and statically profiling
the target application to extract its basic blocks and control flow
information. A basic block (BBL) of code consists of a sequence of instructions
that has only one entry point and, in our case, a single exit point. This means
that no instruction within a BBL is the target of a jump or branch instruction,
and the block is only exited after its last instruction executes. These
properties are desirable for various types of analysis, like the ones performed
by compilers. The control flow information describes how the basic blocks are
linked. The combination of dynamic and static profiling provides us with a
significant part of the CFG, including the part that dominates in terms of
execution time, and would benefit the most from optimization.
%
The analyzer receives the profiler information and extracts data dependencies
from the code, separating program from data tracking logic. It then transforms
the latter to an internal representation, based on the Taint Flow Algebra
(\tfa), which is highly amenable to various optimizations. The optimizations
performed by the analyzer and include classic compiler optimizations like
dead-code elimination and copy propagation as well as DFT specific ones. Our
goal is to remove redundant tracking operations, and reduce the number of
locations where tracking code is inserted. 
%
Finally, the analyzer emits optimized tracking code, which is applied on the
application. Note that the type of tracking code generated depends on the
original tracking implementation to be optimized. Implementation, as it
operates on binary programs the analyzer produces primitive C code, which can
be compiled and inserted into the application using a DBI framework.

\tfa extends \libdft re-using many features such as instruction interpretation
and shadow memory design. More importantly, \libdft plays a role of slow-path
DFT implementation to cover execution paths missed from static analysis phase
ensuring the completeness of our approach.
 
\subsubsection{\sreplica}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.64\linewidth]{figs/architecture.eps}
    \caption{The architecture of \sreplica \label{fig:approach_overview}}
\end{figure}

Figure~\ref{fig:approach_overview} depicts the architecture of \sreplica, which
comprises of two stages. The first stage, shown in the left of the figure
involves profiling an application both statically and dynamically to extract
code blocks, or BBLs, and control-flow information (CFG+). The latter includes
a partial control-flow graph showing how the extracted BBLs are connected, and
frequency data indicating which branches are taken more frequently than others.

This data is processed to generate optimized code to be injected in the
application, and code for running the analysis in parallel. The first contains
code stubs that enqueue the information required to decouple DFT in a shared
data structure. Note that \sreplica does not naively generate code for
enqueueing everything, but ensures that only information that has potentially
changed since the previous executed block are enqueued. This is one of our main
contributions, and problems for previous works~\cite{} that failed to
satisfy equation (1). The second includes code stubs that dequeue information
along with the analysis code.

The generated code is passed to the runtime component of ShadowReplica, shown
in Figiure~\ref{fig:approach_overview}~(b). We utilize a DBI framework that
allows us to inject the enqueueing stubs in the application in an efficient
manner and with flexibility (i.e., on arbitrary instructions of a binary). Our
motivation for using a DBI is that it allows us to apply \sreplica on
unmodified binary applications, and it enables different analyses, security
related or others, by offering the ability to {\it interfere} with the application
at the lowest possible level.

Application threads are executing over the DBI and our tool, which inject the
enqueueing stubs. We will refer to an application thread as the primary. For
each primary, we spawn a shadow thread that will run the analysis code, which
we will refer to as the secondary. While both threads are in the same address
space, applications threads are running over the DBI’s VMM, but shadow threads
are executing natively, since the code generated in the first phase includes
everything required to run the analysis. Our current design spawns secondary
threads in the same process used by the DBI and the application. In the future,
we are considering hosting the secondary threads in a different process for
increased isolation.

Communication between primary and secondary threads is done through a
ring-buffer structure optimized for multi-core architectures . The ring buffer
is also used for the primary thread to synchronize with the secondary, when it
is required that the analysis is complete before proceeding with execution. For
instance, ensuring that integrity has not been compromised before allowing a
certain system call or performing a computed branch.

Finally, we export any new BBLs and CFG edges that are discovered at runtime,
which can be passed back for code analysis. Extending the coverage of our
analysis means that we can generate optimal code for a larger part of the
application. Note that our analysis also generates generic code for handling
application code not discovered during profiling. This “default” code performs
all necessary functionality, albeit slower than the optimized code generated
for known BBLs and control-flow edges.

\subsubsection{Inline vs. Decoupled DFT}
\label{sec:inlinevsdecoupled}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.65\linewidth]{figs/decoupling.eps}
    \caption{Inline vs.decoupled application of DFT application with \sreplica
    and binary instrumentation.\label{fig:decoupling}}
\end{figure}

Dynamically applying DFT on binaries usually involves the use of a dynamic
binary instrumentation (DBI) framework or a virtual machine monitor (VMM) that
will transparently extend the program being analyzed. Such frameworks enable us
to inject code implementing DFT in binaries, by interleaving framework and DFT
code with application code, as shown in
Fig.\ref{fig:decoupling}~(\textit{in-line}) and \libdft and \tfa falls in this
category.

\sreplica proposes an efficient approach for accelerating dynamic DFT and similar
analyses by decoupling them from execution and utilizing spare CPU cores to run
the instrumented application and DFT code in parallel. We replace the in-line
DFT logic in the application with a stub that \emph{extracts} the minimal
information required to independently perform the analysis in another thread,
and \emph{enqueues} the information in a shared data structure. The DFT code,
which is running on a different CPU core, is prefixed with a consumer stub that
\emph{pulls out} the information and then \emph{performs} the analysis.

Decoupling the analysis from execution enables us to run it completely
independently and without involving the instrumentation framework, as
illustrated in Fig.~\ref{fig:decoupling}~(\textit{decoupled}). Depending on the
cost of the analysis (\eg tracking implicit information flows is more costly
than explicit flows), it can accelerate both application and analysis.  In
short, if $I_{i}$, $A_{i}$, and $P_{i}$ are the instrumentation, analysis, and
application code costs with in-line analysis, and $I_{d}$, $A_{d}$, $P_{d}$,
$E_{d}$ and $D_{d}$ are the costs of instrumentation, analysis, application,
enqueueing and dequeueing code (as defined in the above paragraph), then
decoupling is efficient when:
\begin{equation} \vspace{-5pt}
	I_{i} + A_{i} + P_{i} > max(I_{d} + P_{d} + E_{d}, A_{d} + D_{d})
	\label{eqn:efficiency}
\end{equation}
\vspace{-8pt}

Essentially, decoupling is more efficient when the following two conditions are
met: \textit{(a)} if the cost of the in-line analysis is higher than the cost of
extracting the information and enqueueing, and \textit{(b)} if the cost of
program execution combined with instrumentation interference is higher than
dequeueing cost.  Ha~\etal~\cite{cab:oopsala2009} provide a more extensive model
of the costs and benefits involved with decoupling analysis.

Analyses that are bulky code-wise can experience even larger benefits because
replacing them with more compact code, as decoupling does, exerts less
pressure to the instrumentation framework, due to the smaller number of
instructions that need to be interleaved with application code.
For instance, when implementing DFT using binary instrumentation, the
developer needs to take extra care to avoid large chunks of analysis code and
conditional statements to achieve good performance~\cite{libdft:2012vee}. When
decoupling DFT, we no longer have the same limitations, we could even use
utility libraries and generally be more flexible.

We need to emphasize that \sreplica does \emph{not} rely on complete execution
replay~\cite{aftersight:atc2008, paranoidandroid:acsac10} or duplicating
execution in other cores through speculative execution~\cite{speck:asplos2008,
superpin:cgo2007}. So even though other cores may be utilized, it does not
waste processing cycles.  Application code runs exactly once, and the same
stands for the analysis code that runs in parallel. The performance and energy
conservation benefits gained are solely due to exploiting the true parallelism
offered by multi-cores, and being very efficient in collecting and
communicating all the data required for the analysis to proceed independently.

\subsubsection{Evaluation for Previous Systems}

Fig.~\ref{fig:s2k6} represents the performance results that compares previous
DFT implementations against SPEC 2006 CPU benchmarks suite. Given that SPEC is
a CPU intensive one, on average, \libdft, \tfa, \sreplica show $\times 11.27$,
$\times 6.36$, $\times 2.75$ slowdown over native executions.
%
From this result, items that show particularly high slowdown are ones that
incur high instrumentation overhead incurred by underlying DBI framework. {\tt
h264} typically involves many operations for re-prefixed-string instructions
(\eg {\tt rep movs, rep cmps, rep lods, rep stos}). For these instructions, our
underlying instrumentation framework \ie \pin assumes that developer may want
to instrument one of these repetitions, so it JITs these instructions into
explicit loops, allowing the instrumentation to be inserted into each one of
the iterations. The JITed explicit loop is far less performant than the
original rep-prefixed-string instruction. 
%
And {\tt perl} is a language runtime that generates instructions dynamically as
the runtime interprets statements to be executed. Whenever \pin encounters
dynamically generated code, it needs to re-interpret the code and store them in
its code-cache. As a result, not being able to have high cache-hit ratios, DBI
based instrumentations failing to show good performance result when it is
applied the language runtimes. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figs/s2k6.eps}
    \caption{Inline vs.decoupled application of DFT application with \sreplica
    and binary instrumentation.\label{fig:s2k6}}
\end{figure}

Besides SPEC 2006, our systems are extensively evaluated against many mature,
real world applications which include MySQL, Apache, and web browsers. For
details about these result, readers can refer to our previous papers.

We want to discuss further about a couple of interesting insights that we
learned from our most recent DFT implementation or \sreplica and its
performance and efficiency evaluations. Firstly, Figure~\ref{fig:sreplica0}
provides segmented view that illustrates \sreplica's overhead components. From
the left to the right, {\tt NULL\_PIN} represents the pure instrumentation
overhead imposed by DBI instrumentation. In this case, each benchmark items are
executed with \pin DBI but with no analysis logics. Thus additional slowdown of
$\times 0.69$ accounts for the pure instrumentation cost. The next entry of
{\tt SEC\_NULL} represents the slowdown when the enqueueing operation is
instrumented with the application thread without having the analysis thread
that would consumes transferred messages. Thus the additional $\times 1.02$
slowdown is to collect informations to be transferred. Lastly, the analysis
thread is attached to perform DFT operation in {\tt SEC\_DFT} (\ie, full-scale
\sreplica). Interestingly, it only shows $\times 0.4$ of additional slowdown.
It means that, in most cases, the overall slowdown is bound to the application
thread instrumented with event collector and the cost for DFT analysis thread
and communications are not major bottlenecks.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.33\linewidth]{figs/sreplica0.eps}
    \caption{Inline vs.decoupled application of DFT application with \sreplica
    and binary instrumentation.\label{fig:sreplica0}}
\end{figure}

On of the goals of \sreplica was to also make DFT more efficient
computation-wise. In other words, we not only desired to accelerate DFT, but
also do it using less CPU resources. To evaluate this aspect of our approach,
we chose two benchmarks from the SPEC CPU2006 suite; {\tt bzip2} and {\tt
perl}.  Our choice was was not arbitrary. We run \sreplica and our accelerated
in-line DFT implementation with these two benchmarks, and measured their CPU
usage using the {\it perf} tool. Figure~\ref{fig:task0} presents the results of
our experiment. We run \sreplica with both application and analysis threads
running, having the analysis perform no analysis (No analysis), implementing
DFT using all optimizations (DFT), and without the FastPath optimization from
LIFT~\cite{lift}(DFT (NO FP)). The last column(in-line DFT) shows the result
for DFT implementation that inlines the analysis to the application
process~\cite{tfa}. CPU usage is partitioned to show the amount of CPU cycles
taken from the application and analysis threads separately. The darker
horizontal line visualizes the tipping point where the DFT analysis thread
starts dominating performance (\ie it is slower than the application), when we
are running \sreplica with DFT and all optimizations enabled.  A take-out from
these results is that the aggregated CPU usage of \sreplica is less or equal
than that of in-line DFT analysis, \tfa in this case. In the case of {\tt
perl}, we are so much more efficient that we require ∼30\% less CPU cycles to
apply DFT.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/task0.eps}
    \caption{Inline vs.decoupled application of DFT application with \sreplica
    and binary instrumentation.\label{fig:task0}}
\end{figure}
