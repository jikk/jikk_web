% !TEX root = proposal.tex
\section{Related Work}
\label{sec:related}

\subsection{Inline Monitors}
\label{ssec:inline}

Program protection and profiling using inline monitors is a dynamic approach
that executes specific analysis logics along with the application process.
Instances of this technology includes data flow tracking (DFT), memory
integrity checking~\cite{memcheck, drmemory:cgo2011}, control flow
integrity~\cite{cfi}, method counting, call graph profiling \etc. Given that
the technology can be implemented by interleaving analysis and monitoring
logics into the program execution, we can choose to instrument either  source
code or program binary and therefore use different frameworks to implement
inline monitors.

When instrumenting the source code, we can make use of compiler internal
representations~\cite{llvm:cgo04} such as abstract syntax tree (AST) or
intermediate representation (IR) or use source-to-source
transformation~\cite{txl, cil} to generate new source code embedded with
monitoring logics. This approach induces reasonable overhead of roughly
$\times2$ or less, but it is limited in coverage since COTS binaries (\ie 3rd
party libraries) are not supported. An alternative that overcomes this
limitation is to leverage binary instrumentation based on either process-wide
virtualization using dynamic binary instrumentation (DBI)~\cite{pin:pldi2005,
dynamorio, valgrind} or system-wide virtualization~\cite{qemu:usenix05,
xen:sosp2003}. However, it comes with an excessive amount of overhead which
varies from $\times 5 \sim \times 100$ based on analyses and application
domains. Hardware assisted implementation~\cite{raksha:isca2007, lba:isca2008}
is considered to implement inline monitors with minimal overhead usually less
than 5\% with full coverage of execution environment, however the we have not
seen this being supported by major vendors in their commodity products yet.

\subsection{Data Flow Tracking (DFT)}

DFT is one of inline monitoring approaches whose purpose is to accurately
tracks selected data of interest, as they flow during program execution. Among
other uses, DFT has been employed to provide insight in the behavior of
applications and systems, and to assist in the identification of configuration
errors. Most prominently, it has been used in the security field to defend
against various software exploits~\cite{taintcheck:ndss05, dta++:ndss11,
argos:eurosys06, minemu:raid11, taintpolicy:usenixsec06}, and to enforce
information flow by monitoring and restricting the use of sensitive
data~\cite{taintdroid:osdi10, paranoidandroid:acsac10}. For the former, the
network is usually defined as the source of interesting or “tainted” data,
while the use of tainted data is disallowed in certain program locations (\eg,
in instructions manipulating the control flow of programs, such as indirect
branch instructions and function calls). For the latter, the developer or the
user is responsible for specifying the data that needs to be tracked and the
restrictions on their use. DFT is even used to assist solving performance
problems~\cite{xray:osdi2012} pairing configuration file entries with
performance bottlenecks as sources and sinks. 

DFT is implemented by having shadow context that corresponds to the original
execution context. The shadow context comprise of shadow operations and shadow
memory area. Table~\ref{tab:dft_tracking} shows how instructions' DFT semantics
for general purpose instruction architecture (ISA) are defined to perform DFT
operations to keep track of changes from shadow memory area. 

\begin{table}[h]
        \centering
\begin{tabular}{|l|l|}
\hline
{\bf Instruction} & {\bf Tag propagation rule} \\ \hline \hline
    {\tt \specialcell{ALU-OP OP1 $\leftarrow$ OP2 \\ (add, sub \dots)}} & 
    {\tt t(op1) $\vert=$ t(op2)}\\ \hline
    {\tt MOV OP1  $\leftarrow$  OP2} & {\tt t(op1) = t(op2)}     \\ \hline
    {\tt LOAD OP1 $\leftarrow$ [OP2]} & {\tt t(OP1) = t([OP2])}  \\ \hline
    {\tt STORE [OP1] $\leftarrow$ OP2} & {\tt t([OP1]) = t(OP2)} \\ \hline
\end{tabular}
\caption{presents an interpretation of DFT semantics for pseudo instruction set
architecture.}
\label{tab:dft_tracking}
\end{table}

The specifics of DFT can vary significantly depending on ones goals,
performance considerations, and deployment platform. One possible
classification of existing mechanisms can be made based on the means by which
the tracking logic is augmented on regular program execution. As we discussed
from Section~\ref{ssec:inline}, DFT can be performed by inserting data tracking
logic statically during the compilation of software, or by performing source-
to-source code transformation~\cite{txl, cil}. It can also be applied
dynamically by augmenting instrumentation code on existing binaries using
dynamic binary instrumentation (DBI)~\cite{pin:pldi2005, valgrind, dynamorio}
or a modified virtual machine (VM)~\cite{qemu:usenix05, xen:sosp2003}.
Finally, DFT can be also performed in hardware~\cite{raksha:isca2007,
lba:isca2008}.

\subsection{Parallelized Analysis}
\label{ssec:parallel}

The idea of decoupling dynamic program analyses from execution, to run them in
parallel for better response time, has been studied in past in various
contexts. 
%
Aftersight~\cite{aftersight:atc2008}, ReEmu~\cite{reemu:ppopp2013}, and
Paranoid Android~\cite{paranoidandroid:acsac10} leverage record and replay for
recording execution and replaying it, along with the analysis, on a remote host
or a different CPU (replica). They are mostly geared toward off-line analyses
and can greatly reduce the overhead imposed on the application.  However, the
speed of the analysis itself is not improved, since execution needs to be
replayed and augmented with the analysis code on the replica.
SuperPin~\cite{superpin:cgo2007} and Speck~\cite{speck:asplos2008} use
speculative execution to run application and (in-lined) analysis code in
multiple threads that execute in parallel.  These systems sacrifice significant
processing power to achieve speed up.  Furthermore, handling multi-threaded
applications without hardware support remains a challenging issue for this
approach. CAB~\cite{cab:oopsala2009} and PiPA~\cite{pipa:cgo2008} aim at
offloading the analysis code alone to another execution thread. However,
neither of the two has been able to deliver the expected performance gains, due
to {\it (a)} naively collecting information from the application, and {\it (b)}
the high overhead of communicating it to the analysis thread(s). 

\section{Previous Research}
\label{sec:previous}
\subsection{\libdft}

\libdft is a highly optimized DFT framework that shows comparable to or faster
performance than most previous DFT implementations. \libdft implements
instruction level monitors by instrumenting DFT instruction that perform shadow
operations against {\tt x86} binary stream at runtime using PIN
DBI~\cite{pin:pldi2005} framework.
%
\libdft's performance improvement comes in two folds. 

The first optimization is achieved by having {\it inline-friendly} DFT
operations. To interleave codes of two different contexts, underlying DBI
framework should add management/scheduling instructions to save and restore
states for each context switch. The context includes CPU register state  and
the operation of spilling/re-filling these contexts is typically expensive.
{\it Instrumentation inlining}~\cite{inlining:wbia2006} is an approach aims to
reduce this overhead taking advantage of register entries unused from the
context of application execution. In order to make DFT operations to be
instrumentation suitable for inlining, it needs to meet the following two
conditions.
%
\begin{enumerate} \item The instruction count for DFT operation should less
than ten.  \item DFT operation should not contain branch operations that make
updates to {\tt EFLAG} register.  \end{enumerate} 
%
Having highly crafted routines that satisfy both conditions, \libdft could have
most of its operations in-lined to see signification amount of overhead.

The second optimization is by having efficient shadow memory design that
minimizes the cycles needed to translate real address entries into shadow
memory counterparts. We have number of design choices regarding shadow store
structure that come as CPU time vs. memory space trade-offs. Not to reduce the
execution transparency by assign too large portion of address space, most of
shadow memory architectures leverage multiple level of indirections which
inevitably involve conditional operations~\cite{}. \libdft avoids this problem
by instrumentation every memory allocators (\ie {\tt malloc(), mmap(), free()})
to co-allocate/deallocate counterpart shadow entries. This design choice enable
us not only to reduce the instruction counts but also to avoid the usage of
control instruction in translation process.

\libdft takes the form of a shared library enabling developers to create of
DFT-enabled Pintools for binaries, using its extensive API. For example,
\libdft already includes a DTA tool, which can be used to protect applications
from remote buffer overflow exploits.

\subsection{\tfafull (\tfa)} 

Main insights behind \tfa is that update-to-date DFT are lacking in {\it (a)}
consideration for global context {\it (b)} understanding of DFT operation
semantics which is different from the original execution semantics.  \tfa
attempts to address these issues by dedicating off-line static phase that
performs application and DFT specific analysis. 

\begin{figure}[tb]
    \centering
%        \includegraphics[width=\linewidth]{figs/execution_model.pdf}
    \includegraphics[width=0.65\linewidth]{figs/overview_model.eps}
    \caption{\tfa overview: It extract basic blocks and control flow
    information using a combination of dynamic and static analysis, and then
    statically analyze this information to produce optimized data tracking
    code.
   \label{fig:approach_overview}}
\end{figure}

Figure~\ref{fig:approach_overview} shows a high-level overview of our approach
for optimizing data tracking. We start by dynamically and statically profiling
the target application to extract its basic blocks and control flow
information. A basic block (BBL) of code consists of a sequence of instructions
that has only one entry point and, in our case, a single exit point. This means
that no instruction within a BBL is the target of a jump or branch instruction,
and the block is only exited after its last instruction executes. These
properties are desirable for various types of analysis, like the ones performed
by compilers. The control flow information describes how the BBLs are linked.
The combination of dynamic and static profiling provides us with a significant
part of the CFG, including the part that dominates in terms of execution time,
and would benefit the most from optimization.
%
The analyzer receives the profiler information and extracts data dependencies
from the code, separating program from data tracking logic. It then transforms
the latter to an internal representation, based on the Taint Flow Algebra
(\tfa), which is highly amenable to various optimizations. The optimizations
performed by the analyzer and include classic compiler optimizations like
dead-code elimination and copy propagation as well as DFT specific ones. Our
goal is to remove redundant tracking operations, and reduce the number of
locations where tracking code is inserted. 
%
Finally, the analyzer emits optimized tracking code, which is applied on the
application. Note that the type of tracking code generated depends on the
original tracking implementation to be optimized. Implementation, as it
operates on binary programs the analyzer produces primitive C code, which can
be compiled and inserted into the application using a DBI framework.

\tfa extends \libdft re-using many of its features such as instruction
interpretation and shadow memory architecture. More importantly, \libdft plays
a role of slow-path DFT implementation to cover execution paths missed from
static analysis phase ensuring the completeness of our approach.
 
\subsection{\sreplica}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.64\linewidth]{figs/architecture.eps}
    \caption{The architecture of \sreplica \label{fig:sreplica_arch}}
\end{figure}

Figure~\ref{fig:sreplica_arch} depicts the architecture of \sreplica, which
comprises of two stages. The first stage, shown in the left of the figure
involves profiling an application both statically and dynamically to extract
code blocks, or BBLs, and control-flow information (CFG+). The latter includes
a partial control-flow graph showing how the extracted BBLs are connected, and
frequency data indicating which branches are taken more frequently than others.

This data is processed to generate optimized code to be injected in the
application, and code for running the analysis in parallel. The first contains
code stubs that enqueue the information required to decouple DFT in a shared
data structure. Note that \sreplica does not naively generate code for
enqueueing everything, but ensures that only information that has potentially
changed since the previous executed block are enqueued. This is one of our main
contributions, and problems for previous works~\cite{speck:asplos2008,
superpin:cgo2007, pipa:cgo2008} that failed to satisfy equation (1). The second
includes code stubs that dequeue information along with the analysis code.

The generated code is passed to the runtime component of ShadowReplica, shown
in Figiure~\ref{fig:approach_overview}~(b). We utilize a DBI framework that
allows us to inject the enqueueing stubs in the application in an efficient
manner and with flexibility (i.e., on arbitrary instructions of a binary). Our
motivation for using a DBI is that it allows us to apply \sreplica on
unmodified binary applications, and it enables different analyses, security
related or others, by offering the ability to {\it interfere} with the application
at the lowest possible level.

Application threads are executing over the DBI and our tool, which inject the
enqueueing stubs. We will refer to an application thread as the primary. For
each primary, we spawn a shadow thread that will run the analysis code, which
we will refer to as the secondary. While both threads are in the same address
space, applications threads are running over the DBI’s VMM, but shadow threads
are executing natively, since the code generated in the first phase includes
everything required to run the analysis. Our current design spawns secondary
threads in the same process used by the DBI and the application. In the future,
we are considering hosting the secondary threads in a different process for
increased isolation.

Communication between primary and secondary threads is done through a
ring-buffer structure optimized for multi-core architectures . The ring buffer
is also used for the primary thread to synchronize with the secondary, when it
is required that the analysis is complete before proceeding with execution. For
instance, ensuring that integrity has not been compromised before allowing a
certain system call or performing a computed branch.

Finally, we export any new BBLs and CFG edges that are discovered at runtime,
which can be passed back for code analysis. Extending the coverage of our
analysis means that we can generate optimal code for a larger part of the
application. Note that our analysis also generates generic code for handling
application code not discovered during profiling. This “default” code performs
all necessary functionality, albeit slower than the optimized code generated
for known BBLs and control-flow edges.

\subsubsection{Inlined vs. Decoupled DFT}
\label{sec:inlinevsdecoupled}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/decoupling.eps}
    \caption{Inline vs.decoupled application of DFT application with \sreplica
    and binary instrumentation.\label{fig:decoupling}}
\end{figure}

Dynamically applying DFT on binaries usually involves the use of a dynamic
binary instrumentation (DBI) framework or a virtual machine monitor (VMM) that
will transparently extend the program being analyzed. Such frameworks enable us
to inject code implementing DFT in binaries, by interleaving framework and DFT
code with application code, as shown in
Fig.\ref{fig:decoupling}~(\textit{in-line}) and \libdft and \tfa falls in this
category.

\sreplica proposes an efficient approach for accelerating dynamic DFT and similar
analyses by decoupling them from execution and utilizing spare CPU cores to run
the instrumented application and DFT code in parallel. We replace the in-line
DFT logic in the application with a stub that \emph{extracts} the minimal
information required to independently perform the analysis in another thread,
and \emph{enqueues} the information in a shared data structure. The DFT code,
which is running on a different CPU core, is prefixed with a consumer stub that
\emph{pulls out} the information and then \emph{performs} the analysis.

Decoupling the analysis from execution enables us to run it completely
independently and without involving the instrumentation framework, as
illustrated in Fig.~\ref{fig:decoupling}~(\textit{decoupled}). Depending on the
cost of the analysis (\eg tracking implicit information flows is more costly
than explicit flows), it can accelerate both application and analysis.  In
short, if $I_{i}$, $A_{i}$, and $P_{i}$ are the instrumentation, analysis, and
application code costs with in-line analysis, and $I_{d}$, $A_{d}$, $P_{d}$,
$E_{d}$ and $D_{d}$ are the costs of instrumentation, analysis, application,
enqueueing and dequeueing code (as defined in the above paragraph), then
decoupling is efficient when:
\begin{equation} \vspace{-5pt}
	I_{i} + A_{i} + P_{i} > max(I_{d} + P_{d} + E_{d}, A_{d} + D_{d})
	\label{eqn:efficiency}
\end{equation}
\vspace{-8pt}

Essentially, decoupling is more efficient when the following two conditions are
met: \textit{(a)} if the cost of the in-line analysis is higher than the cost of
extracting the information and enqueueing, and \textit{(b)} if the cost of
program execution combined with instrumentation interference is higher than
dequeueing cost.  Ha~\etal~\cite{cab:oopsala2009} provide a more extensive model
of the costs and benefits involved with decoupling analysis.

Analyses that are bulky code-wise can experience even larger benefits because
replacing them with more compact code, as decoupling does, exerts less
pressure to the instrumentation framework, due to the smaller number of
instructions that need to be interleaved with application code.
For instance, when implementing DFT using binary instrumentation, the
developer needs to take extra care to avoid large chunks of analysis code and
conditional statements to achieve good performance~\cite{libdft}. When
decoupling DFT, we no longer have the same limitations, we could even use
utility libraries and generally be more flexible.

We need to emphasize that \sreplica does \emph{not} rely on complete execution
replay~\cite{aftersight:atc2008, paranoidandroid:acsac10} or duplicating
execution in other cores through speculative execution~\cite{speck:asplos2008,
superpin:cgo2007}. So even though other cores may be utilized, it does not
waste processing cycles.  Application code runs exactly once, and the same
stands for the analysis code that runs in parallel. The performance and energy
conservation benefits gained are solely due to exploiting the true parallelism
offered by multi-cores, and being very efficient in collecting and
communicating all the data required for the analysis to proceed independently.

\subsection{Evaluation Results}
\label{ssec:prev_eval}

Fig.~\ref{fig:s2k6} represents the performance results that compares previous
DFT implementations against SPEC 2006 CPU benchmarks suite. Given that SPEC is
a CPU intensive one, on average, \libdft, \tfa, \sreplica show $\times 11.27$,
$\times 6.36$, $\times 2.75$ slowdown over native executions.
%
From this result, items that show particularly high slowdown are ones that
incur high instrumentation overhead to underlying DBI framework. {\tt h264}
typically involves many operations for re-prefixed-string instructions (\eg
{\tt rep movs, rep cmps, rep lods, rep stos}). For these instructions, our
underlying instrumentation framework of \pin assumes that developer may want to
instrument one of these repetitions, so it JITs these instructions into
explicit loops, allowing the instrumentation to be inserted into each one of
the iterations. The JITed explicit loop is far less performant than the
original rep-prefixed-string instruction. 
%
And {\tt perl} is a language runtime that generates codes to be executed
on-the-fly as the runtime interprets statements. Whenever \pin encounters
dynamically generated code, it needs to re-translate the code and store them in
its code-cache for later usage. This kind of operation being too frequent and
not being able to maintain high cache-hit ratio, PIN DBI fails to have good
performance result.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figs/s2k6.eps}
    \caption{Inline vs.decoupled application of DFT application with \sreplica
    and binary instrumentation.\label{fig:s2k6}}
\end{figure}

Besides SPEC 2006 CPU benchmark suite, our prototypes are extensively evaluated
against many mature, real world applications which include MySQL, Apache, and
web browsers. For details about these result, readers can refer to our previous
papers.

We want to discuss further about a couple of interesting insights that we
learned in the course of developing \sreplica for its performance and
efficiency results. Firstly, Figure~\ref{fig:sreplica0} illustrates \sreplica's
overhead segmentation. From the left to the right, {\tt NULL\_PIN} represents
the pure instrumentation overhead imposed by DBI instrumentation. In this case,
each benchmark items are executed atop \pin DBI but with no analysis logic
instrumented.  Thus averaged slowdown of $\times 0.69$ accounts for the pure
instrumentation cost. The next entry of {\tt SEC\_NULL} represents the slowdown
when the enqueueing operation is instrumented with the application thread
without having the analysis thread that would consumes transferred messages.
Thus the additional $\times 1.02$ slowdown denotes the event collector cost to
gather and transit relevant informations from the application thread.  Lastly,
the analysis thread is attached to perform DFT operation in {\tt SEC\_DFT}
(\ie, full-scale \sreplica).  Interestingly, it only shows $\times 0.4$ of
additional slowdown. It means that, in most cases, the overall slowdown is
bound to the application thread instrumented with event collector and DFT
analysis runs faster than the application thread preventing communications and
DFT analysis from being a bottleneck.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.33\linewidth]{figs/sreplica0.eps}
    \caption{represents peformance breakdown of \sreplica implementation.
    \label{fig:sreplica0}}
\end{figure}

Another goal of \sreplica implementation was to also make DFT more efficient
computation-wise. In other words, we not only desired to accelerate DFT, but
also do it using less CPU resources. To evaluate this aspect of our approach,
we chose two benchmarks from the SPEC CPU2006 suite; {\tt bzip2} and {\tt
perl}.  Our choice was was not arbitrary. We run \sreplica and our accelerated
in-line DFT implementation (\tfa) with these two benchmarks, and measured their
CPU usage using the {\it perf} tool. Figure~\ref{fig:task0} presents the
results of our experiment. We run \sreplica with both application and analysis
threads running, having the analysis perform no analysis (No analysis),
implementing DFT using all optimizations (DFT), and without the FastPath
optimization from LIFT~\cite{lift:2006micro}(DFT (NO\_FP)). The last
column(in-line DFT) shows the result for DFT implementation that inlines the
analysis to the application process~\cite{tfa:ndss2012}. CPU usage is
partitioned to show the amount of CPU cycles taken from the application and
analysis threads separately. The darker horizontal line visualizes the tipping
point where the DFT analysis thread starts dominating performance (\ie it is
slower than the application), when we are running \sreplica with DFT and all
optimizations enabled.  A take-out from these results is that the aggregated
CPU usage of \sreplica is less or equal than that of in-line DFT analysis, \tfa
in this case. In the case of {\tt perl}, we are so much more efficient that we
require ∼30\% less CPU cycles to apply DFT.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/task0.eps}
    \caption{Inline vs.decoupled application of DFT application with \sreplica
    and binary instrumentation.\label{fig:task0}}
\end{figure}
